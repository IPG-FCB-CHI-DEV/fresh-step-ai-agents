# ------------------------------------
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
# ------------------------------------

import time
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from openai.types.eval_create_params import DataSourceConfigCustom

from test_utils import retrieve_agent, retrieve_endpoint, retrieve_model_deployment, Colors


def test_evaluation():
    with (
        DefaultAzureCredential(exclude_interactive_browser_credential=False) as credential,
        AIProjectClient(endpoint=retrieve_endpoint(), credential=credential) as project_client,
        project_client.get_openai_client() as openai_client,
    ):

        agent = retrieve_agent(project_client)
        model = retrieve_model_deployment()

        data_source_config = DataSourceConfigCustom(
            type="custom",
            item_schema={"type": "object", "properties": {"query": {"type": "string"}}, "required": ["query"]},
            include_sample_schema=True,
        )

        # Define testing criteria. Explore the evaluator catalog for more built-in evaluators.
        testing_criteria = [
            # quality evaluation of agent messages (sample.output_items)
            {
                "type": "azure_ai_evaluator",
                "name": "task_completion",
                "evaluator_name": "builtin.task_completion",
                "data_mapping": {
                    "query": "{{item.query}}",
                    "response": "{{sample.output_items}}"
                },
                "initialization_parameters": {"deployment_name": f"{model}"},
            },
            {
                "type": "azure_ai_evaluator",
                "name": "task_adherence",
                "evaluator_name": "builtin.task_adherence",
                "data_mapping": {
                    "query": "{{item.query}}",
                    "response": "{{sample.output_items}}"
                },
                "initialization_parameters": {"deployment_name": f"{model}"},
            },
            {
                "type": "azure_ai_evaluator",
                "name": "tool_call_success",
                "evaluator_name": "builtin.tool_call_success",
                "data_mapping": {
                    "response": "{{sample.output_items}}"
                },
                "initialization_parameters": {"deployment_name": f"{model}"},
            },
            # safety evalution of agent responses (sample.output_text)
            {
                "type": "azure_ai_evaluator",
                "name": "violence",
                "evaluator_name": "builtin.violence",
                "data_mapping": {
                    "query": "{{item.query}}",
                    "response": "{{sample.output_text}}"
                },
            },
            {
                "type": "azure_ai_evaluator",
                "name": "indirect_attack",
                "evaluator_name": "builtin.indirect_attack",
                "data_mapping": {
                    "query": "{{item.query}}", 
                    "response": "{{sample.output_text}}"
                },
            },      
        ]

        eval_object = openai_client.evals.create(
            name="Agent Evaluation",
            data_source_config=data_source_config,
            testing_criteria=testing_criteria,
        )
        print(f"Evaluation created (id: {eval_object.id}, name: {eval_object.name})")

        # Define data source for evaluation run
        data_source = {
            "type": "azure_ai_target_completions",
            "source": {
                "type": "file_content",
                "content": [
                    {"item": {"query": "Tell me a joke about a robot"}},
                    {"item": {"query": "What are the best places to visit in Tokyo?"}},
                ],
            },
            "input_messages": {
                "type": "template",
                "template": [
                    {"type": "message", "role": "user", "content": {"type": "input_text", "text": "{{item.query}}"}}
                ],
            },
            "target": {
                "type": "azure_ai_agent",
                "name": agent.name,
                "version": agent.version,  # Version is optional. Defaults to latest version if not specified
            },
        }

        # Submit evaluation run
        run = openai_client.evals.runs.create(
            eval_id=eval_object.id, name=f"Evaluation Run for Agent {agent.name}", data_source=data_source
        )
        print(f"Evaluation run created (id: {run.id})")

        # Poll for completion
        while run.status not in ["completed", "failed"]:
            run = openai_client.evals.runs.retrieve(run_id=run.id, eval_id=eval_object.id)
            print(f"Waiting for eval run to complete... current status: {run.status}")
            time.sleep(5)

        assert run.status == "completed", "Evaluation run did not complete successfully!"
        print(f"\n{Colors.GREEN}Evaluation run completed successfully!")
 
        if run.result_counts.errored > 0:
            print(f"{Colors.RED}Error items: {run.result_counts.errored}")

        if run.result_counts.failed > 0:
            print(f"{Colors.RED}Failed items: {run.result_counts.failed}")

        print(f"{Colors.YELLOW}Review evaluation results in this report:")
        print(f"{Colors.CYAN}{run.report_url}\n")

        Colors.reset()


if __name__ == "__main__":
    test_evaluation()